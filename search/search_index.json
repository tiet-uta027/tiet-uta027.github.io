{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>UTA027: Artificial Intelligence TIET Patiala</p>"},{"location":"#course-website","title":"Course Website","text":"<p>Instructors:</p> <ol> <li><code>(  RGB)</code> Raghav B. Venkataramaiyer <code>&lt;bv.raghav&gt;</code></li> <li><code>(  STT)</code> Stuti Chug <code>&lt;stuti.chug&gt;</code></li> <li><code>(  ABJ)</code> Anu Bajaj <code>&lt;anu.bajaj&gt;</code></li> <li><code>(PTKRA)</code> Parteek Saini <code>&lt;psaini_phd24&gt;</code></li> </ol> <ol> <li>Requires <code>thapar.edu</code> login</li> </ol>"},{"location":"#1academic-calendar","title":"<sub><sup>(1)</sup></sub>Academic Calendar","text":""},{"location":"#course-syllabus","title":"Course Syllabus","text":"<p>Download </p>"},{"location":"#evaluation","title":"Evaluation","text":"Code Title Date Weightage SESS#A1 Assignment 1 CE 10 SESS#A2 Assignment 2 CE 10 SESS#QZ1 Quiz 1 18-02-2025 1730 IST 05 MST Mid Sem Exam TBA 25 SESS#QZ2 Quiz 2 06-05-2025 1730 IST 05 EST End Sem TBA 45"},{"location":"#schedule-of-lectures","title":"Schedule of Lectures","text":""},{"location":"#orientation","title":"Orientation","text":"<ol> <li>     Overview and    administrative details. </li> </ol>"},{"location":"#predicate-calculus-mst-est","title":"Predicate Calculus [MST]\u00a0\u00a0[EST]","text":"<ol> <li>    Predicate Calculus    Introduction to Predicate Logic and Representation    of Knowledge and Heuristics</li> <li> Reasoning    (Predicate Calculus)</li> <li>Application to Knowledge Graphs and Lifecycle of a    research problem</li> </ol> <p>See also: A01</p>"},{"location":"#the-graphical-standpoint-mst-est","title":"The Graphical Standpoint [MST]\u00a0\u00a0[EST]","text":"<ol> <li>     Graph Theory +      BFS/DFS.</li> <li> Dijkstra\u2019s     Algorithm for    Single-Source Shortest Path. </li> <li> Problem solving from a    graphical stand point</li> </ol>"},{"location":"#classical-ml-mst-est","title":"Classical ML [MST]\u00a0\u00a0[EST]","text":"<ol> <li> Introduction to ML</li> <li> Linear Regression</li> <li> Classification and Logistic    Regression</li> <li> Support Vector    Machines</li> </ol> <p>See Also: ML Notes or, Download </p>"},{"location":"#neural-methods-mst-est","title":"Neural Methods [MST]\u00a0\u00a0[EST]","text":"<ol> <li> Neuron and it application in    Regression/ Classification</li> <li> Deep Neural Networks</li> <li>Deep Learning: Residuals, Gating and RNN\u2019s</li> <li>State Machines and Reinforcement Learning</li> </ol>"},{"location":"#computer-vision-est","title":"Computer Vision [EST]","text":"<ol> <li> Computer Vision Overview</li> <li> Computer Vision Problems</li> <li>Deep Learning in Computer Vision:     Lecture Set 1,     Lecture Set 2.</li> </ol>"},{"location":"#introduction-to-advanced-topics-est","title":"Introduction to Advanced Topics  [EST]","text":"<ol> <li>Attention</li> <li>Diffusion</li> <li>Inverse Rendering</li> </ol>"},{"location":"#schedule-of-assignments","title":"Schedule of Assignments<sup>1</sup><sup>2</sup><sup>3</sup>","text":"S.No. Desc/Link Deadline A01 Predicate Calculus 20-01-2025 0500 IST A02 Graph Methods (TBA) Practice: Python and Algos 10-02-2025 0500 IST A03 Linear Regression 24-02-2025 0500 IST A04 Neuron-based Regression 24-Mar to 4-Apr A05 Neural Regression (Iris Dataset) 7-Apr to 18-Apr A06 Visual Object Classification 21-Apr to 2-May A07 YOLO 4-May to 15-May A08 Visual Object Detection 21-04-2025 0500 IST A09 GAN based Generative AI 28-04-2025 0500 IST A10 Proposals 05-05-2025 0500 IST"},{"location":"#resources","title":"Resources","text":"<ol> <li>[CL] The Central Library    (Link)</li> <li>[RR] RefRead    (Link)</li> <li>[TB] [CL] [RR]     Luger, G. F. &amp; others. (1998). Artificial    intelligence: Structures and strategies for complex    problem solving (Sixth). Pearson Education    India. <code>ISBN: 9788131743744</code></li> <li>[TB] [CL] Bishop,    C. M. (2006). Pattern recognition and machine    learning. Springer. <code>ISBN: 9788132209065</code></li> <li>[RB] [CL] Cormen, T. H.,    Leiserson, C. E., Rivest, R. L., &amp; Stein,    C. (2022). Introduction to Algorithms (Fourth). MIT    Press. <code>ISBN: 9788120340077</code></li> <li>[RB] [CL] Gareth James,    Daniela Witten, Trevor Hastie, &amp; Robert    Tibshirani. (2013). An Introduction to Statistical    Learning (1st ed.). Springer. <code>DOI:    10.1007/978-1-4614-7138-7</code> <code>ISBN: 9781461471387</code> (Link)</li> <li>[RB] [CL] MacKay,    D. J. C. (2003). Information theory, inference and    learning algorithms. Cambridge University    Press. <code>ISBN: 9780521670517</code> (Link)</li> <li>[RB] Bertsekas, D., &amp; Tsitsiklis,    J. N. (2008). Introduction to probability    (Vol. 1). Athena Scientific. <code>ISBN: 9781886529236</code> (Google Scholar)</li> <li>[YT] [MOOC] Introduction    to Probability. (MIT-OCW) (Archive    2011) (Archive 2018)</li> <li>[YT] [MOOC] Algorithms    Illuminated. by Tim Roughgarden Videos: Part 1    Basics, Videos: Part 2    Graphs and Official Website</li> <li>[RB] [CL] Jurafsky, D.,    &amp; Martin, J. H. (2025, January). Speech and Language    Processing: An Introduction to Natural Language    Processing, Computational Linguistics, and Speech    Recognition. <code>ISBN: 9789332518414</code> (The    Book), (The Chapter on Logistic    Regression), (Official    Website)</li> <li>[MOOC] Illinois Institute Page on     Logistic Regression.</li> <li>[YT] Late Prof. Winston\u2019s Lecture on     SVM (MIT-OCW) Video by MIT-OCW</li> </ol> <ol> <li> <p>Each assignment carries weightage of 2 marks\u00a0\u21a9</p> </li> <li> <p>Some of the assignments are competition and leaderboard based; Marks awarded shall be based on rank on leaderboard.\u00a0\u21a9</p> </li> <li> <p>A01-05 A01-03 shall reflect on the webkiosk as consolidated <code>SESS#A1 (MM:10)</code>; and similary A06-10 A04-07 as <code>SESS#A2 (MM:10)</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"a04/","title":"A04","text":"<p> Download this page as PDF</p>"},{"location":"a04/#synthetic-dataset","title":"Synthetic Dataset","text":"<p>Define a synthetic dataset \\(\\mathcal{D}\\) (in 2D) so that,</p> \\[\\begin{align*}   y = ce^{ax} + \\epsilon \\end{align*}\\] <p>Where,  \\(\\epsilon\\) is a random noise.  \\(a,c\\) are arbitrary constants, and  \\(0 \\leqslant x,y \\leqslant 1\\)</p>"},{"location":"a04/#neuron","title":"Neuron","text":"<p>Define a neuron parameterised by weight \\(w\\), bias \\(b\\) and activation function \\(\\mathcal{A}\\), so that</p> \\[\\begin{align*}   f(x;w,b,\\mathcal{A}) = \\mathcal{A}(wx+b) \\end{align*}\\]"},{"location":"a04/#gradient-descent","title":"Gradient Descent","text":"<p>Define a training loop for learning the parameters of a neural network (a neuron here) for regression.  Let the parameters be \\(w,b\\) and given are the dataset \\(\\mathcal{D}\\) and activation function \\(\\mathcal{A}\\).</p> <p>Pseudocode for Training Loop:</p> <pre><code>Given :\n  + X,Y :: Dataset\n  + A :: Activation Function\n  + dA :: Derivative of Activation Function\n  + l :: learning rate\nInitialise :\n  + w,b :: NN Params with random values.\nLoop until convergence:\n  Y_hat = [A(wx+b) Forall x in X]\n  Err = [(y_hat-y) Forall (y_hat,y) in {Y_hat,Y}]\n  L = average(Err^2)\n  dLdw = # TODO: compute dLdw here\n  dLdb = # TODO: compute dLdb here\n  w = w - l*dLdw\n  b = b - l*dLdb\nReturn :\n  w,b,L\n</code></pre>"},{"location":"a04/#radial-basis-activation","title":"Radial Basis Activation","text":"<p>Use the following activation function to learn the NN Params:</p> \\[\\begin{align*}   \\mathcal{A}(x) &amp;= e^{-\\frac{x^2}{2}} \\\\   \\frac{\\partial\\mathcal{A}}{\\partial x}(x)   &amp;= -x\\mathcal{A}(x) \\end{align*}\\]"},{"location":"a05/","title":"A05","text":"<p> Download this page as PDF</p> <p>Assignment 05: Multi-Layer Neural Network on the Iris Dataset Using PyTorch  (07-Apr to 18-Apr)</p>"},{"location":"a05/#objective","title":"Objective","text":"<p>The goal of this assignment is to implement a multi-layer neural network (MLP) using PyTorch to classify the Iris dataset.</p>"},{"location":"a05/#question","title":"Question","text":"<p>Design and implement a multi-layer neural network using PyTorch to classify the Iris dataset. Your implementation should follow these steps:</p>"},{"location":"a05/#dataset-preparation","title":"Dataset Preparation","text":"<ul> <li>Load the Iris dataset using     sklearn.datasets.load<sub>iris</sub>.</li> <li>Convert the dataset into PyTorch tensors.</li> <li>Split the dataset into training and test sets (e.g.,     80% training, 20% testing).</li> <li>Normalize the feature values.</li> </ul>"},{"location":"a05/#build-the-neural-network-model","title":"Build the Neural Network Model","text":"<ul> <li>Implement an MLP with PyTorch using     <code>torch.nn.Module</code>.</li> <li>The model should have:<ul> <li>An input layer with 4 neurons (one for each     feature).</li> <li>At least one hidden layer with ReLU activation.</li> <li>An output layer with 3 neurons (one for each class)     and softmax activation.</li> </ul> </li> </ul>"},{"location":"a05/#train-the-model","title":"Train the Model","text":"<ul> <li>Define the loss function (CrossEntropyLoss).</li> <li>Choose an optimizer (e.g., Adam or SGD).</li> <li>Train the model for a fixed number of epochs (e.g.,     100 epochs).</li> <li>Track the loss during training.</li> </ul>"},{"location":"a05/#evaluate-the-model","title":"Evaluate the Model","text":"<p>Compute accuracy on the test set.  Generate a confusion matrix to visualize performance.</p>"},{"location":"a05/#theory","title":"Theory","text":"<ul> <li> Neuron     and it application in Regression/ Classification.</li> </ul>"},{"location":"a05/#boilerplate-code","title":"Boilerplate Code","text":"<ul> <li> +      How to create a neural network     module.</li> <li> +      How to create a pytorch dataset from     tensors.</li> </ul>"},{"location":"a05/#evaluation-criterion","title":"Evaluation Criterion","text":"<p>This assignment shall be implemented by students and evaluated by the instructor in lab.</p>"},{"location":"a06/","title":"A06","text":"<p> Download this page as PDF</p>"},{"location":"a06/#dataset-preparation","title":"Dataset Preparation","text":"<ul> <li>Use <code>torchvision.datasets.VOCDetection</code> to load the     Pascal VOC 2007 or 2012 dataset.</li> <li>Extract image-level labels from annotations.</li> <li>Convert the dataset into PyTorch tensors.</li> <li>Split the dataset into training and validation sets     (e.g., 80% training, 20% validation).</li> <li>Resize and normalize the image data.</li> </ul>"},{"location":"a06/#data-visualization","title":"Data Visualization","text":"<ul> <li>Display at least 5 sample images with their bounding     boxes and labels.</li> <li>Plot a bar chart showing the frequency of each object     class.</li> <li>Plot a pie chart of the top 5 most common classes.</li> </ul>"},{"location":"a06/#build-the-classification-model","title":"Build the Classification Model","text":"<ul> <li>Choose a pre-trained model from torchvision.models     (e.g., ResNet, VGG).</li> <li>Replace the final layer with a new fully connected     layer to output predictions for 20 classes.</li> <li>Use ReLU activation in hidden layers and softmax     (implicitly handled by loss function) for the output.</li> </ul>"},{"location":"a06/#train-the-model","title":"Train the Model","text":"<ul> <li>Define a suitable loss function (e.g.,     CrossEntropyLoss).</li> <li>Choose an optimizer (e.g., Adam or SGD).</li> <li>Train the model for a fixed number of epochs (e.g.,     10\u201320 epochs).</li> <li>Track and display training and validation loss per     epoch.</li> </ul>"},{"location":"a06/#evaluate-the-model","title":"Evaluate the Model","text":"<ul> <li>Compute accuracy, precision, and recall on the     validation set.</li> <li>Display a confusion matrix to visualize     classification performance.</li> </ul>"},{"location":"a07/","title":"A07","text":"<p> Download this page as PDF</p> <p>A07 : Object Detection with YOLO 5-May/12-May</p>"},{"location":"a07/#introduction","title":"Introduction","text":""},{"location":"a07/#background","title":"Background","text":"<p>Object detection is a computer vision technique that identifies and locates objects within images or videos.   It goes beyond image classification by drawing bounding boxes around detected objects, enabling machines to \"see\" and understand their environment.   This is crucial for applications like autonomous driving, surveillance, robotics, and medical imaging, where identifying object locations is essential for decision-making and automation.</p> <p>The YOLO (You Only Look Once) family is a popular series of object detection models known for their speed and efficiency.  YOLOv8 is a cutting-edge version that offers state-of-the-art performance.</p> <p>YOLO models are particularly well-suited for this task because they process the entire image in a single pass through the neural network.  This design choice makes them significantly faster than other object detection methods, enabling real-time performance.  Despite their speed, YOLO models, especially YOLOv8, achieve high accuracy, making them ideal for applications requiring both speed and precision.</p>"},{"location":"a07/#learning-objectives","title":"Learning Objectives","text":"<ol> <li>Understand the YOLO architecture.</li> <li>Set up a development environment for YOLO.</li> <li>Prepare a dataset for YOLO training.</li> <li>Train a pre-trained YOLO model on a custom dataset.</li> <li>Evaluate the performance of a trained YOLO model.</li> <li>Perform object detection on images/videos using the     trained model.</li> </ol>"},{"location":"a07/#dataset","title":"Dataset","text":"<p>Use the Pascal VOC Dataset which is available off the shelf with both PyTorch and TensorFlow ecosystem.</p> <p>The PASCAL VOC dataset is a standard dataset for object detection tasks. It defines a specific set of object classes that models are trained to detect. The classes in the PASCAL VOC dataset are:</p> <ul> <li>Person: <code>person</code></li> <li>Animal: <code>bird</code>, <code>cat</code>, <code>cow</code>, <code>dog</code>, <code>horse</code>,     <code>sheep</code></li> <li>Vehicle: <code>aeroplane</code>, <code>bicycle</code>, <code>boat</code>, <code>bus</code>,     <code>car</code>, <code>motorbike</code>, <code>train</code></li> <li>Indoor: <code>bottle</code>, <code>chair</code>, <code>dining table</code>, <code>potted       plant</code>, <code>sofa</code>, <code>tv/monitor</code></li> </ul> <p>Dataset preparation and annotation are crucial for the PASCAL VOC dataset's effectiveness in training robust object detection models. High-quality annotations, including accurate bounding boxes and class labels, enable models to learn precise object locations and categories. Consistent annotation across the dataset ensures that models generalize well to unseen images.</p> <p>Proper dataset preparation, such as splitting the data into training, validation, and test sets, is essential for evaluating model performance and preventing over-fitting.</p>"},{"location":"a07/#software-and-libraries","title":"Software and Libraries","text":"<ol> <li>Python 3.x</li> <li>PyTorch or TensorFlow (specify your framework of     choice).</li> <li><code>torchvision</code> (if using PyTorch) or     <code>tensorflow-datasets</code> (if using TensorFlow).</li> <li><code>ultralytics</code> (for YOLOv5 or v8).</li> <li>Recommended: Using a GPU for training.</li> </ol>"},{"location":"a07/#tasks","title":"Tasks","text":"<p>Starter Code: [IPython Notebook on Gist]\u200b</p>"},{"location":"a07/#task-1-training-the-yolo-model","title":"Task 1: Training the YOLO Model","text":"<ol> <li>Train the YOLO Model on the VOC Dataset.</li> <li>Pay special attention to training configuration,     documented here, or here, and also listed under the     title \u201cRef <code>model.train</code> options,\u201d in the starter     code.</li> <li>Modify/ Update it to achieve better performance.</li> </ol>"},{"location":"a07/#task-2-model-evaluation","title":"Task 2: Model Evaluation","text":"<ol> <li> <p>Evaluate the model under the following metrics:</p> <ul> <li>Precision</li> <li>Recall</li> <li>Intersection over Union (IoU)</li> <li>Mean Average Precision (mAP)</li> </ul> <p>The candidates are encouraged to use library functions e.g. PyTorch Eval Library, instead of rolling off their own implementations. 2.  Analyse the results and discuss the model's strengths and weaknesses. 3.  Explain the confusion matrix.</p> </li> </ol>"},{"location":"a07/#task-3-inference-and-visualization","title":"Task 3: Inference and Visualization","text":"<ol> <li>Capture 10-20 images from \u201cyour daily life\u201d to     qualitatively assess the performance of your model.</li> <li>Visualise the results.</li> <li>Analyse the results.</li> </ol>"},{"location":"a07/#tips","title":"Tips","text":"<ol> <li>You are encouraged to train the model in multiple     runs successively improving upon it by parts,     instead of a single pass.</li> <li>Remember to save the model in a persistent storage     device, lest your endeavours be in vain.</li> <li>Consider to resume training from an earlier     checkpoint.  Remember that your runtime might be     limited with free cloud compute!  E.g. on Google     Colab you may monitor the estimated available     runtime under \u201cView Resources.\u201d</li> </ol>"},{"location":"a07/#submission","title":"Submission","text":"<ul> <li>Code files (Python scripts or Jupiter Notebooks)</li> <li>A report (in PDF format) that includes:<ul> <li>Introduction</li> <li>Methodology (explaining the data preparation,     training process, and evaluation methods)</li> <li>Results (including tables and figures showing the     evaluation metrics)</li> <li>Discussion (analysing the results, discussing     challenges, and suggesting future improvements)</li> </ul> </li> <li>Trained model weights.</li> </ul>"},{"location":"a07/#additional-tips-for-students","title":"Additional Tips for Students","text":"<ul> <li>Provide links to relevant documentation and     tutorials that have been of help.</li> <li>You are encouraged to use version control (Git).</li> <li>Use a consistent coding style.</li> <li>Comment the code clearly.</li> </ul>"},{"location":"graph-theory/","title":"Graph theory","text":"<p> Download this page as PDF</p>"},{"location":"graph-theory/#graph-representations","title":"Graph Representations","text":""},{"location":"graph-theory/#fundamentals","title":"Fundamentals","text":"<p>(See slides)</p>"},{"location":"graph-theory/#question-1","title":"Question 1","text":"<p>Are graphs useful in real world?  Support your answer with examples from your domain.</p>"},{"location":"graph-theory/#question-2","title":"Question 2","text":"<p>What is the difference between adjacency list and adjacency matrix?  Cite real world examples of usage.</p> <p>Hint: List is compact, whereas Matrix is detailed. List makes more sense in sparse graphs, and Matrix in dense ones.</p>"},{"location":"graph-theory/#question-3","title":"Question 3","text":"<p>If \\(G(V,M)\\) represents a given graph \\(G\\) with a set of vertices \\(V\\) and adjacency matrix \\(M\\), what is the significance of a transpose graph \\(G^{\\top}(V,M^{\\top})\\)?  Cite examples from your domain for emphasis.</p> <p>Hint: The edges in \\(G^{\\top}\\) are reversed when compared against those in \\(G\\).  (See also: this question )</p>"},{"location":"graph-theory/#question-4","title":"Question 4","text":"<p>Comment on the nature of graph \\(G(V,M+M^{\\top})\\) where \\(M\\) is a square matrix with empty diagonals.</p> <p>Hint: If \\(M\\) consists of forward edges, then it follows that \\(M^{\\top}\\) corresponding are reverse edges; hence \\(M+M^{\\top}\\) consists of bi-directional edges.  (See also: this question )</p>"},{"location":"graph-theory/#vertex-insertion","title":"Vertex Insertion","text":""},{"location":"graph-theory/#question","title":"Question","text":"<p>\\(\\overline{\\mathit{ABCD}}\\) is a closed quadrilateral. A new vertex \\(E\\) is introduced between \\(B\\) and \\(C\\). Show the adjacency lists before and after the introduction of \\(E\\).  Hence, write an algorithm/ pseudocode in order to introduce a new vertex between an existing edge.</p>"},{"location":"graph-theory/#interpretation","title":"Interpretation","text":"<p>Given a closed quadrilateral \\(\\overline{\\mathit{ABCD}}\\), the adjacency list in 1-indexed format is given as:</p> <pre><code>V = [A, B, C, D]\nAdj = [[2, 4],                  # 1 is connected to 2 and 4\n       [1, 3],                  # 2 is connected to 1 and 3\n       [2, 4],                  # and so forth\n       [1, 3]]\n</code></pre> <p>Here the edge \\(\\overline{\\mathit{BC}}\\) is defined in two entries of the adjacency list, i.e. as vertex <code>3</code> in <code>Adj[2]</code> and vertex <code>2</code> in <code>Adj[3]</code>.</p>"},{"location":"graph-theory/#solution","title":"Solution","text":"<p>In order to introduce a new vertex \\(E\\) between edge \\(\\overline{\\mathit{BC}}\\),</p>"},{"location":"graph-theory/#step-1","title":"Step 1","text":"<p>Append vertex \\(E\\) to the vertex list \\(V\\) and get its index.</p> <pre><code>V = [A, B, C, D, E]             # Add E as V[5]\n</code></pre>"},{"location":"graph-theory/#step-2","title":"Step 2","text":"<p>Remove the edge  \\(\\overline{\\mathit{BC}}\\)</p> <pre><code>V = [A, B, C, D, E]\nAdj = [[2, 4],\n       [1],                     # remove 3 from Adj[2]\n       [4],                     # remove 2 from Adj[3]\n       [1, 3],\n       []]                      # add empty Adj[5]\n</code></pre>"},{"location":"graph-theory/#step-3","title":"Step 3","text":"<p>Add edges \\(\\overline{\\mathit{BEC}}\\)</p> <pre><code>V = [A, B, C, D, E]\nAdj = [[2, 4],\n       [1, 5],                  # add 5 to Adj[2]\n       [4, 5],                  # add 5 to Adj[3]\n       [1, 3],\n       [2, 3]]                  # add 2, 3 to Adj[5]\n</code></pre>"},{"location":"graph-theory/#algorithm","title":"Algorithm","text":"<p>To introduce a vertex \\(W\\) between an edge \\((u,v)\\),</p> <pre><code>GRAPH_ADD_VERTEX_BW(G,W,u,v) :\n  w = G.V.append(W)             # Insert W into list of\n                                # vertices and store the\n                                # last appended index.\n\n  G.Adj[u].remove(v)            # Remove v from Adj[u]\n  G.Adj[v].remove(u)            # Remove u from Adj[v]\n\n  G.Adj[u].append(w)            # Append w into Adj[u]\n  G.Adj[v].append(w)            # Append w into Adj[v]\n\n  G.Adj[w].append(u)            # Append u into Adj[w]\n  G.Adj[w].append(v)            # Append v into Adj[w]\n</code></pre> <p>PS: Here, \\(W\\) in uppercase refers to a variable (i.e. vertex information like coordinates of a point etc.) that needs to appended into the list of verts \\(G.V\\). And \\((u,v)\\) represent the indices of the pair of verts that constitute and edge.  We are interested in the Adjacency List (as required by the question,) hence the use of \\(G.Adj\\)</p>"},{"location":"graph-theory/#transpose-graph","title":"Transpose Graph","text":""},{"location":"graph-theory/#question_1","title":"Question","text":"<p>Given a graph \\(G(V,M)\\), \\(M\\) being the adjacency matrix. A transpose graph would be the one with same set of vertices, but a transposed adjacency matrix, i.e. \\(G^{\\top}(V,M^{\\top})\\).  What does a transpose graph represent?  Illustrate with a drawing to support your answer.</p>"},{"location":"graph-theory/#interpretation_1","title":"Interpretation","text":"<p>Recall that,</p> <ol> <li>In an adjacency matrix \\(A\\), the component at     \\(i^{\\text{th}}\\) row, and \\(j^{\\text{th}}\\) column is     given as \\(a_{ij}\\) and it represents whether the edge     \\(v_{i}\\to v_{j}\\) exists.</li> <li>A transpose graph \\(G^{\\top}(V,M^{\\top})\\) would be     any different, iff \\(M\\ne M^{\\top}\\).  In other     words, if \\(G\\) is a directed graph.</li> <li>The components in the transposed matrix are mirrored     across the diagonal.  Hence, if \\(B = A^{\\top}\\), then     \\(b_{ij} = a_{ji}\\).</li> </ol>"},{"location":"graph-theory/#solution_1","title":"Solution","text":"<p>Each edge \\(v_{i}\\to v_{j}\\) in \\(G\\), transforms to \\(v_{j}\\to v_{i}\\) in the transpose graph \\(G^{\\top}\\). In other words, the edges are reversed.</p> <p>This would be any different, only in case of a directed graph.  Since for an undirected graph \\(M=M^{\\top}\\). Hence, the transpose graph \\(G^{\\top}(V,M^{\\top})\\) represents \\(G(V,M)\\) with edges reversed.</p>"},{"location":"graph-theory/#illustration","title":"Illustration","text":"\\[\\begin{align*}   M = \\begin{bmatrix}     0&amp;1&amp;0\\\\0&amp;0&amp;1\\\\1&amp;0&amp;0   \\end{bmatrix}\\quad M^{\\top} = \\begin{bmatrix}     0&amp;0&amp;1\\\\1&amp;0&amp;0\\\\0&amp;1&amp;0   \\end{bmatrix}\\quad M+M^{\\top} = \\begin{bmatrix}     0&amp;1&amp;1\\\\1&amp;0&amp;1\\\\1&amp;1&amp;0   \\end{bmatrix} \\end{align*}\\] <p> Graph and its Transpose </p>"},{"location":"graph-theory/#inout-degree","title":"(In/Out)-degree","text":""},{"location":"graph-theory/#question_2","title":"Question","text":"<p>What is the average in-degree of a graph \\(G(V,E)\\), where \\(E\\) is the set of edges in \\(G\\)?</p>"},{"location":"graph-theory/#solution_2","title":"Solution","text":"<p>In-degree of a vertex is defined as the number of edges leading onto itself.</p> <p>Let \\(d_{\\mathrm{in}}(v)\\) represent the in-degree of vertex \\(v\\).  Then the average in-degree is given as the sum of in-degrees divided by the size of number of verts,</p> \\[\\begin{align*}   \\mathbb{E}[d_{\\mathrm{in}}(v)]   &amp;= \\frac{\\sum_{v\\in V}d_{\\mathrm{in}}(v)} {|V|} \\end{align*}\\] <p>Intuitively speaking, the sum of all in-degrees is the same as the number of edges. Hence,</p> \\[\\begin{align*}   \\mathbb{E}[d_{\\mathrm{in}}(v)]   &amp;= \\frac{|E|} {|V|} \\end{align*}\\]"},{"location":"graph-theory/#in-further-detail","title":"In further detail","text":"<p>In-degree of a vertex is the same as counting the non-zeros in one (specific) column of an adjacency matrix representation \\(M\\) for the set of edges \\(E\\).</p> <p>Similarly, the sum \\(\\sum_{v\\in V}d_{\\mathrm{in}}(v)\\) is equivalent to</p> <ul> <li>Counting the non-zeros for every the column of \\(M\\),</li> <li>i.e. Counting all the non-zeros in \\(M\\),</li> <li>i.e. The number of edges.</li> </ul> <p>Hence,</p> \\[\\begin{align*}   \\sum_{v\\in V} d_{\\mathrm{in}}(v)     &amp;= |E| \\end{align*}\\]"},{"location":"graph-theory/#representation","title":"Representation","text":""},{"location":"graph-theory/#question_3","title":"Question","text":"<p>Provide an adjacency list as well as the adjacency matrix representation for trees A and B in the following figure.</p> <p></p> <p>Tree A</p> <p></p> <p>Tree B</p>"},{"location":"graph-theory/#solution_3","title":"Solution","text":""},{"location":"graph-theory/#tree-a","title":"Tree A","text":"<pre><code>Adj = [[2 3] \n       [1 4 5] \n       [1 6 7] \n       [2]\n       [2]\n       [3]\n       [3]]\n\nM = [[0 1 1 0 0 0 0]\n     [1 0 1 1 0 0 0]\n     [1 0 0 0 1 1 0]\n     [0 1 0 0 0 0 0]\n     [0 1 0 0 0 0 0]\n     [0 0 1 0 0 0 0]\n     [0 0 1 0 0 0 0]]\n</code></pre>"},{"location":"graph-theory/#tree-b","title":"Tree B","text":"<pre><code>Adj = [[2]\n       [1 3 4]\n       [2]\n       [2 6]\n       [6]\n       [4 5 7]\n       [6]]\nM = [[0 1 0 0 0 0 0]\n     [1 0 1 1 0 0 0]\n     [0 1 0 0 0 0 0]\n     [0 1 0 0 0 1 0]\n     [0 0 0 0 0 1 0]\n     [0 0 0 1 1 0 1]\n     [0 0 0 0 0 1 0]]\n</code></pre>"},{"location":"graph-theory/#ps","title":"PS","text":"<p>The Adjacency matrix of Tree B is bi-symmetric.</p>"},{"location":"graph-theory/#elementary-algorithms","title":"Elementary Algorithms","text":""},{"location":"graph-theory/#fundamentals_1","title":"Fundamentals","text":""},{"location":"graph-theory/#question-1_1","title":"Question 1","text":"<p>Cite examples to highlight the difference between when and why to prioritise the use of BFS in stead of DFS, and vice-versa.</p>"},{"location":"graph-theory/#question-2_1","title":"Question 2","text":"<p>What operations are performed while visiting a vertex \\(v\\) during BFS.  How are they different from visiting a vertex \\(v\\) during DFS?</p> <p>Hint: One iteration of while loop is a visit during BFS, whereas visit in a DFS finishes only after all successors have been visited.</p> <p>PS: The answer to this question is also the difference between stacked and queued operations in general.</p>"},{"location":"graph-theory/#question-3_1","title":"Question 3","text":"<p>Visiting a vertex in BFS assigns three vertex properties.  Describe them highlighting their importance in problem solving in general.</p> <p>Hint: Discovery time is also the distance from source; and following the parent is the shortest path to source.</p>"},{"location":"graph-theory/#question-4_1","title":"Question 4","text":"<p>How is DFS useful in real world?  Emphasise the significance of discovery and finish times in your example?</p>"},{"location":"graph-theory/#question-5","title":"Question 5","text":"<p>At any instant, during the DFS, how does the colour of a node, help determining the edge classification?</p> <p>Hint: See this slide</p>"},{"location":"graph-theory/#question-6","title":"Question 6","text":"<p>How can DFS be used to detect cycles in a graph? Comment.</p>"},{"location":"graph-theory/#question-7","title":"Question 7","text":"<p>How can parenthesis structure of a DFS help determine dependencies and relationships?</p> <p>Variants:</p> <ol> <li> <p>A project is subdivided into tasks, and it has been     understood that some tasks are dependent upon     others.  How would you determine if one task must be     completed before another?  Write an algorithm/     pseudocode for the same.</p> <p>Hint: ``One task must be completed before another\u2019\u2019 implies an order.</p> </li> <li> <p>Given a large family database, with parent-child     relationships between individuals, how would you     determine if at all related (directly), Jaspreet is     ancestor/descendant of Dilraj?</p> <p>Hint: Parenthesis structure exhibits direct relationships.</p> </li> </ol> <p>See Also: this question</p>"},{"location":"graph-theory/#bfs","title":"BFS","text":""},{"location":"graph-theory/#question_4","title":"Question","text":"<p> Graph A </p> <p>With reference to Graph A  Determine algorithmically,</p> <ol> <li>The shortest path weight \\(\\delta(u,j)\\) for the pair     \\((u,j)\\) of vertices.</li> <li>A shortest path between the pair \\((u,j)\\) of     vertices.</li> <li>All shortest-paths originating from vertex \\(u\\).</li> </ol> <p></p> <p> BFS on Graph A </p>"},{"location":"graph-theory/#key-insight","title":"Key Insight","text":"<p>All the three questions here speak about a shortest path originating from vertex \\(u\\).  This is a uniformly weighted undirected graph, i.e. all edges are equally weighted.  The solution for shortest path will follow a BFS in such a case.</p>"},{"location":"graph-theory/#solution_4","title":"Solution","text":"<ol> <li>Running a BFS on the graph gives us the figure, \u201cBFS     on Graph A\u201d      upon termination.</li> <li>The numbers marked are discovery times of the nodes  \\(v\\cdot d \\ \\forall v\\in V\\).</li> <li>For part (1) the shortest path weight is given as     \\(\\delta(u,j) = j\\cdot d - u\\cdot d\\).  Computing from     the figure, \\(\\delta(u,j) = 4-0 = 4\\).</li> <li> <p>For part (2) we may pick any one path such that each     successive node is from successive level.  i.e.     one of,</p> <ol> <li>\\(\\langle u,v,h,i,j\\rangle\\),</li> <li>\\(\\langle u,v,w,y,j\\rangle\\), or</li> <li>\\(\\langle u,v,x,y,j\\rangle\\).</li> </ol> <p>Recall, that only one of these is, and not all of them are, the required shortest path (i.e. discovered in one run).</p> </li> <li> <p>For part (3), a BFS tree is required.  It\u2019s been     left that upon the reader to exercise and present as     necessary.  An easy way out would be to use the     adjoining graph      and     additionally mark each connection from \u201cparent\u201d to     \u201cchild\u201d as descended during the BFS.  Note that the     arrow would be a manifestation of line <code>v.PI = u</code> in     the algorithm (link to the slide).  Recall that     there may be only one parent to a child, not many,     and that the discovery time of the parent is always     less than that of the child.</p> </li> </ol>"},{"location":"graph-theory/#dfs","title":"DFS","text":""},{"location":"graph-theory/#question_5","title":"Question","text":"<p>Given that there are 10 courses in a programme, and corresponding pre-requisites are listed as under, determine algorithmically If the programme may be completed successfully by a candidate?</p> <ol> <li>depends upon 2 and 3;</li> <li>depends upon 3 and 4;</li> <li>depends upon none;</li> <li>depends upon none;</li> <li>depends upon 4 and 6;</li> <li>depends upon none;</li> <li>depends upon 5 and 8;</li> <li>depends upon 4, 6 and 10;</li> <li>depends upon 2, 4 and 8;</li> <li>depends upon 6 and 9.</li> </ol>"},{"location":"graph-theory/#key-insight_1","title":"Key Insight","text":"<p>We define a relationship \\(u\\to v\\) if course \\(u\\) depends upon \\(v\\) (i.e. if course \\(v\\) is a pre-requisite of course \\(u\\)).  Then we get a dependency graph (i.e. a directed graph where relationship is defined when the parent is dependent upon the child).</p> <p>A topological order \\(T\\equiv\\langle v_{1},\\ldots,v_{k} \\rangle\\) of such a graph means that all ancestors of \\(v_{i}\\) have been listed before \\(v_{i}\\) itself \\(\\forall v_{i}\\in V\\).  In simple words, the topological order is one possible order of courses to complete the programme.</p> <p>However, the topological order is not always possible. From our slides, we know that topological order is defined only for a directed acyclic graph (DAG). Hence, one may complete the programme iff the dependency graph is acyclic.</p> <p>And a graph is acyclic if and only if there are no back edges.</p>"},{"location":"graph-theory/#solution_5","title":"Solution","text":"<ol> <li>Run a DFS on Dependency Graph;</li> <li>Maintain a list \\(T\\) for Topological Order;</li> <li>Upon finishing the visit to a node, insert the node     to the front of the list;</li> <li>Exit \u201cabnormally,\u201d if encountered a \u201cback edge.\u201d</li> </ol> <p>If exited abnormally, the graph has a cycle; and the programme can not be completed successfully.</p> <p>Otherwise, the graph is acyclic, and \\(T\\) contains an order of courses that successfully completes the programme.</p> <p>In figure \u201cDFS on Dependency Graph,\u201d  nodes have been mentioned with discovery and finish times; and edges have been labelled as B,C,F,T for back edges, cross edges, forward edges and tree edges respectively.</p> <p>The algorithm terminated upon visiting the edge \\(9\\to 8\\) which is a back edge (labelled B).</p> <p>Hence the programme can not be completed.</p> <p></p> <p> DFS on the Dependency Graph </p>"},{"location":"graph-theory/#dfs-parenthesis-structure","title":"DFS Parenthesis Structure","text":""},{"location":"graph-theory/#question_6","title":"Question","text":"<p>In DFS, discovery and finishing times are indicators of ancestry.  Comment.</p>"},{"location":"graph-theory/#solution_6","title":"Solution","text":"<ol> <li>When DFS discovers a vertex \\(u\\), it marks the     discovery time \\(u\\cdot d\\) (the left parenthesis).</li> <li>When DFS finishes visit to all the neighbours of     \\(u\\), it marks the finishing time \\(u\\cdot f\\) (the     right parenthesis).</li> <li>If \\(u\\) is an ancestor of vertex \\(v\\) in the DFS tree, then,<ul> <li>\\(u\\) was discovered before \\(v\\), i.e. \\(u\\cdot d &lt;          v\\cdot d\\);</li> <li>The visit of \\(u\\) was finished after that of \\(v\\),     i.e. \\(v\\cdot f &lt; u\\cdot f\\); and</li> <li>Thus, discovery/finish interval of \\(u\\) completely     encloses that of \\(v\\).</li> </ul> </li> </ol>"},{"location":"graph-theory/#problem-solving","title":"Problem Solving","text":""},{"location":"graph-theory/#three-jug-problem","title":"Three jug problem","text":""},{"location":"graph-theory/#question_7","title":"Question","text":"<p>There are three unmarked jugs \\(A,B,C\\) with a capacity of 8, 5 and 3 units respectively.  Possible moves may either empty a can into another or fill the other, whichever occurs earlier.  Starting with \\(A8,B0,C0\\), determine algorithmically if and how we can reach to a split of \\(A4,B4,C0\\).</p> <p></p> <p> BFS on State Graph </p>"},{"location":"graph-theory/#key-insight_2","title":"Key Insight","text":"<ol> <li>The jugs are unmarked.  Hence, there is no way to     determine any intermediate quantity while pouring.</li> <li>For every state reachable from any other state, at     least one of the jugs is either empty or full.  This     is a direct consequence of a possible move     (action,) as defined in the problem.</li> <li>Each jug may be poured into the other two, so there     may be 6 actions.  But at every state, at least one     jug is empty or full; the number of actions is     limited to 4.</li> <li>Some moves are reversible, e.g. \\(A8,B0,C0        \\rightleftharpoons A3,B5,C0\\).  As a consequence, the     resultant state graph is cyclic in nature.</li> </ol>"},{"location":"graph-theory/#solution_7","title":"Solution","text":"<ul> <li> <p>State Space: is a 3-vector \\(\\mathbf{v} \\equiv       Aa,Bb,Cc\\) that satisfies,</p> \\[\\begin{align*} \\boldsymbol{0}   \\leqslant \\begin{bmatrix}a&amp;b&amp;c \\end{bmatrix}^{\\top}   &amp; \\leqslant \\begin{bmatrix}8&amp;5&amp;3 \\end{bmatrix}^{\\top}   \\\\   a+b+c &amp;= 8 \\end{align*}\\] </li> <li> <p>Start State: \\(\\mathbf{s}=A8,B0,C0\\)</p> </li> <li> <p>Actions: Pour from jug, until the latter is full,     or else empty the former.</p> </li> </ul> <p>Since the state graph is cyclic in nature, our solution is based out of BFS. See the figure titled, \u201cBFS on State Graph.\u201d</p>"},{"location":"graph-theory/#three-jug-problem-2","title":"Three jug problem 2","text":""},{"location":"graph-theory/#question_8","title":"Question","text":"<p>There are three unmarked jugs \\(A,B,C\\) with a capacity of 8, 5 and 3 units respectively.  Possible moves may either empty a can into another or fill the other, whichever occurs earlier.  Starting with \\(A8,B0,C0\\), can we can reach to a split of \\(A4,B3,C1\\).</p>"},{"location":"graph-theory/#solution_8","title":"Solution","text":"<p>(This is a logical deduction, not an algorithmic solution.)</p> <p>From our key insights (earlier),</p> <p>For every state reachable from any other state, at least one of the jugs is either empty or full.</p> <p>The state \\(A4,B3,C1\\) has neither of the jugs empty, nor full!  Hence this is not a reachable state!</p>"},{"location":"ml-notes/","title":"Ml notes","text":"<p> Download this page as PDF</p>"},{"location":"ml-notes/#setup","title":"Setup","text":"<ol> <li>Given is a set of paired observations \\(\\mathcal{D}\\)     (aka evidence), where targets \\(y\\in\\mathbb{R}\\) are     paired with (\\(d\\) dimensional) features     \\(\\mathbf{x}\\in\\mathbb{R}^{d}\\).</li> <li>We propose a mathematical model (typically a     \u201cfamily\u201d of functions)     \\(\\mathcal{F}_{\\boldsymbol{\\theta}} : \\mathbb{R}^{d}        \\to \\mathbb{R}\\) parameterised by     \\(\\boldsymbol{\\theta}\\)</li> <li>So that \\(y\\approx        \\mathcal{F}_{\\boldsymbol{\\theta}_{*}} (\\mathbf{x})\\).     Here \\(y\\) are referred to as targets,     \\(\\mathcal{F}_{\\boldsymbol{\\theta}} (\\mathbf{x})\\) are     referred to as predictions; so that predictions     approximate the targets, under optimal set of learnt     parameters, \\({\\boldsymbol{\\theta}_{*}}\\).</li> <li> <p>We express this formally as:      Find \\({\\boldsymbol{\\theta} =        {\\boldsymbol{\\theta}_{*}}}\\) in order to</p> \\[\\begin{align*}   \\underset{\\boldsymbol{\\theta}} {\\text{minimise}}   \\quad   &amp;\\underset{y,\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}     \\left[ \\Delta(y, \\mathcal{F}_{\\boldsymbol{\\theta}}     (\\mathbf{x})) \\right] \\end{align*}\\] <p>where, \\(\\Delta\\) is the notion of distance between predictions \\(\\mathcal{F}_{\\boldsymbol{\\theta}}    (\\mathbf{x})\\) and targets \\(y\\).</p> </li> </ol>"},{"location":"ml-notes/#linear-regression","title":"Linear Regression","text":""},{"location":"ml-notes/#in-2d","title":"In 2D","text":"\\[\\begin{align*}   y \\approx \\mathcal{F}_{w,b}(x)   &amp;= wx+b \\\\   \\Delta\\left(y, \\mathcal{F}_{w,b}(x)\\right)   &amp;= \\frac12 \\left(y - \\mathcal{F}_{w,b}(x) \\right)^2 \\end{align*}\\] <p>The objective is to find \\(w=w_*\\), \\(b=b_*\\) in order to</p> \\[\\begin{align*}   \\underset{w,b}{\\text{minimise}}   &amp;\\quad \\underset{y,x\\sim\\mathcal{D}}{\\mathbb{E}}     \\left[ \\frac12 \\left(y - \\mathcal{F}_{w,b}(x)     \\right)^2 \\right] \\end{align*}\\] <p>The analytical solution yields,</p> \\[\\begin{align*}   w_* &amp;= \\frac{\\mathrm{coVar}(x,y)}{\\mathrm{Var}(x)} \\\\   b_* &amp;= \\mathbb{E}[y]-w_*\\mathbb{E}[x] \\\\   \\mathrm{coVar}(x,y) &amp;= \\mathbb{E}[xy] -                         \\mathbb{E}[x]\\mathbb{E}[y] \\\\    \\mathrm{Var}(x) &amp;= \\mathbb{E}[x^2]-\\mathbb{E}^2[x] \\end{align*}\\]"},{"location":"ml-notes/#in-higher-dimensions","title":"In Higher Dimensions","text":"\\[\\begin{align*}   y \\approx \\mathcal{F}_{\\mathbf{w}}(\\mathbf{x})   &amp;= \\mathbf{w}^{\\top}\\mathbf{x} =     \\mathbf{x}^{\\top}\\mathbf{w} \\\\   &amp;= w_0 + w_1x_1 + \\cdots + w_dx_d \\qquad (x_0 = 1) \\\\   \\Delta\\left(y, \\mathcal{F}_{\\mathbf{w}}(\\mathbf{x})   \\right)   &amp;= \\frac12 \\left(y -     \\mathcal{F}_{\\mathbf{w}}(\\mathbf{x}) \\right)^2  \\\\   &amp;= \\frac12 \\left(y -     \\mathbf{x}^{\\top}\\mathbf{w} \\right)^2  \\\\ \\end{align*}\\] <p>The objective is to find \\(\\mathbf{w}=\\mathbf{w}_*\\) in order to</p> \\[\\begin{align*}   \\underset{\\mathbf{w}}{\\text{minimise}}   &amp;\\quad \\underset{y,\\mathbf{x}\\sim\\mathcal{D}}{\\mathbb{E}}     \\left[ \\frac12 \\left(y - \\mathbf{x}^{\\top}\\mathbf{w}     \\right)^2 \\right] \\\\   \\text{or,}\\quad \\underset{\\mathbf{w}}{\\text{minimise}}   &amp;\\quad \\frac12 (\\mathbf{y}-X\\mathbf{w})^{\\top}     (\\mathbf{y}-X\\mathbf{w}) \\\\   \\text{where,}\\quad \\mathbf{y}\\equiv\\begin{bmatrix}     y_1 \\\\ \\vdots \\\\ y_N   \\end{bmatrix} &amp;\\quad X \\equiv \\begin{bmatrix}     \\mathbf{x}_1^{\\top} \\\\ \\vdots \\\\     \\mathbf{x}_N^{\\top}   \\end{bmatrix} \\end{align*}\\] <p>The analytical solution yields,</p> \\[\\begin{align*}   \\mathbf{w}_* &amp;= (X^{\\top}X)^{-1}X^{\\top}\\mathbf{y} \\end{align*}\\]"},{"location":"ml-notes/#implementation","title":"Implementation","text":""},{"location":"ml-notes/#in-spreadsheet","title":"In Spreadsheet","text":"<p>This (Google Sheet) will help understand and practice computing the solution manually for the case in 2D.</p>"},{"location":"ml-notes/#in-code","title":"In Code","text":"<p>This (Gist) is a reference python implementation of the analytical solution.</p>"},{"location":"ml-notes/#logistic-regression","title":"Logistic Regression","text":"<p>(Binary Classification)</p> \\[\\begin{align*}   y \\approx \\widetilde{y} = \\mathcal{F}_{\\mathbf{w}}(\\mathbf{x})   &amp;= \\sigma(\\mathbf{x}^{\\top}\\mathbf{w}) \\\\   \\Delta\\left(y, \\widetilde{y} \\right)   &amp;= -\\left(y\\ln \\widetilde{y} + (1-y)     \\ln (1-\\widetilde{y})\\right) \\\\   \\frac{\\partial \\Delta(y,\\widetilde{y})} {\\partial   \\mathbf{w}}   &amp;= -(y-\\widetilde{y})\\mathbf{x} \\end{align*}\\] <p>The objective is to find \\(\\mathbf{w}=\\mathbf{w}_*\\) in order to</p> \\[\\begin{align*}   \\underset{\\mathbf{w}}{\\text{minimise}}   &amp;\\quad \\mathcal{L}(\\mathbf{w}) = \\underset{y,     \\mathbf{x} \\sim \\mathcal{D}}{\\mathbb{E}}     \\left[ \\Delta\\left(y, \\widetilde{y} \\right) \\right] \\end{align*}\\] <p>There\u2019s no analytical solution.  But using gradient descent, we numerically hope to converge using iterative update,</p> \\[\\begin{align*}   \\mathbf{w} &amp;\\gets \\mathbf{w} -\\lambda \\frac {\\partial                \\mathcal{L}} {\\partial \\mathbf{w}} \\\\              &amp;= \\mathbf{w} + \\lambda \\, \\underset{y,                \\mathbf{x} \\sim \\mathcal{D}}                {\\mathbb{E}} \\left[(y-\\widetilde{y})                \\mathbf{x} \\right] \\end{align*}\\] <p>Corrigendum 2025-02-22 </p> <ol> <li>Typo in Binary Cross Entropy Loss.  Earlier     mentioned: \\(\\Delta=y\\ln\\widetilde{y}+\\cdots\\).  This     has been corrected now.</li> <li>By cascading effect, \\(\\partial \\Delta/\\partial        \\mathbf{w}\\), and the update step for gradient     descent algorithm also have been corrected.</li> </ol>"},{"location":"ml-notes/#support-vector-machine","title":"Support Vector Machine","text":"<ol> <li>Given a dataset \\(\\mathcal{D}\\) with paired samples     \\((y,\\mathbf{x}); y\\in\\{+1,-1\\}\\) so that positive     samples are labeled \\(y=+1\\), and similarly negative     samples as \\(y=-1\\).</li> <li>To evaluate for a simple case, let\u2019s assume that the     positive and negative samples are \u201ccomfortably\u201d     separable through a hyperplane.  In case of 2D     data \\((\\mathbf{x}\\in\\mathbb{R}^2)\\), it would follow     from the assumption that there exists a straight     line with a finite margin, called gutter space     such that,<ol> <li>There are no samples in the gutter space;</li> <li>Positive samples lie on one side of the     hyperplane; and</li> <li>Negative samples lie on the other side.</li> </ol> </li> <li>Our aim is to find the straight line that maximises     the gutter space.</li> <li> <p>Let the separating hyperplane (straight line in case     of 2D data) be given as,</p> \\[\\begin{align}   \\mathbf{w}\\cdot\\mathbf{x} + b = 0 \\end{align}\\] <p>Geometrically speaking, \\(\\mathbf{w}\\) is a vector normal to the separating hyperplane.  And the unit vector in the same direction is given as \\(\\mathbf{w}/\\|\\mathbf{w}\\|_2\\).  Where \\(\\|\\mathbf{w}\\|_2\\) is called the Frobenius Norm and \\(\\|\\mathbf{w}\\|_2^2 = w_1^2+\\cdots+w_d^2\\).  This is the same as the understanding of \u201cmagnitude\u201d of the vector in Euclidean space.</p> </li> <li> <p>The hyperplane separates the space such that      One side of it satisfies     \\(\\mathbf{w}\\cdot\\mathbf{x}+b &lt; 0\\); and      The other side satisfies     \\(\\mathbf{w}\\cdot\\mathbf{x}+b &gt; 0\\).</p> </li> <li> <p>From the separability assumption, it follows,   </p> \\[\\begin{align*}   \\mathbf{w}\\cdot\\mathbf{x}+b &lt; 0 &amp;\\quad\\forall y=-1 \\\\   \\mathbf{w}\\cdot\\mathbf{x}+b &gt; 0 &amp;\\quad\\forall y=+1 \\end{align*}\\] </li> <li> <p>From the margin assumption, without loss of     generality, it follows that   </p> \\[\\begin{align*}    \\mathbf{w}\\cdot\\mathbf{x}+b \\leqslant -1 &amp;\\quad    \\forall y=-1 \\\\    \\mathbf{w}\\cdot\\mathbf{x}+b \\geqslant 1 &amp;\\quad    \\forall y=+1 \\end{align*}\\] </li> <li> <p>In other words</p> \\[\\begin{align}   y(\\mathbf{w}\\cdot\\mathbf{x}+b) \\geqslant 1 \\end{align}\\] </li> <li> <p>For the points on the margin, denoted as     \\(\\mathbf{x}_{+}, \\mathbf{x}_{-}\\) in the adjoining     image,</p> \\[\\begin{align}   \\notag\\mathbf{w}\\cdot\\mathbf{x}_{+} + b &amp;= 1 \\\\   \\notag\\mathbf{w}\\cdot\\mathbf{x}_{-} + b &amp;= -1 \\\\   \\mathbf{w}\\cdot(\\mathbf{x}_{+}-\\mathbf{x}_{-}) &amp;= 2 \\end{align}\\] </li> <li> <p>The gutter width \\(\\gamma\\) is given as the     projection of vector \\(\\mathbf{x}_{+} -         \\mathbf{x}_{-}\\) along the normal to the hyperplane.     Or,</p> \\[\\begin{align}   \\notag   \\gamma &amp;= \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|_2} \\cdot            (\\mathbf{x}_{+}-\\mathbf{x}_{-}) \\\\   \\notag &amp;= \\frac{\\mathbf{w}\\cdot            (\\mathbf{x}_{+}-\\mathbf{x}_{-})}            {\\|\\mathbf{w}\\|_2} \\\\   \\gamma &amp;= \\frac{2}{\\|\\mathbf{w}\\|_2} \\end{align}\\] <p>Our aim is to maximise the gutter width \\(\\gamma\\), which would be the same as minimising \\(1/\\gamma\\), or \\(1/\\gamma^{2}\\), or \\(4/\\gamma^{2} =     \\|\\mathbf{w}\\|_{2}^{2}\\).</p> </li> </ol>"},{"location":"ml-notes/#training","title":"Training","text":"<p>Formally speaking, we need to find the parameters \\(\\mathbf{w},b\\) in order to</p> \\[\\begin{align*}   \\text{minimise} &amp;\\quad\\|\\mathbf{w}\\|_2^2 \\\\   \\text{subject to,} &amp;\\quad y(\\mathbf{w}\\cdot\\mathbf{x}                       + b) \\geqslant 1 \\end{align*}\\]"},{"location":"ml-notes/#inference","title":"Inference","text":"<p>For all unseen points, \\(\\mathbf{x}\\), the estimated label \\(\\widehat{y}\\) is given as,</p> \\[\\begin{align}   \\widehat{y} &amp;= \\mathrm{signum}(\\mathbf{w}\\cdot\\mathbf{x}+b) \\end{align}\\]"},{"location":"ml-notes/#implementation_1","title":"Implementation","text":"<p>Check out this gist</p>"},{"location":"predicates/","title":"Predicates","text":"<p> Download this page as PDF</p>"},{"location":"predicates/#predicate-no-290e","title":"Predicate No. <code>290e</code>","text":""},{"location":"predicates/#well-formed-expression","title":"Well Formed Expression","text":"<p>\\(\\forall X (\\text{student?}(X) \\rightarrow \\exists Y (\\text{book?}(Y) \\land \\text{has\\_read?}(X, Y)))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language","title":"Interpretation in Natural Language","text":"<p>Every student has read some book.</p>"},{"location":"predicates/#predicate-no-d9e2","title":"Predicate No. <code>d9e2</code>","text":""},{"location":"predicates/#well-formed-expression_1","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Feature} (\\exists \\mathit{Library} (\\mathrm{depends\\_on?}(\\mathit{Feature}, \\mathit{Library})) \\rightarrow (\\neg \\mathrm{compatible\\_with?}(\\mathit{Feature}, \\mathit{Library}) \\land \\mathrm{deprecated?}(\\mathit{Library})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_1","title":"Interpretation in Natural Language","text":"<p>Every feature depends upon some incompatible and deprecated library.</p>"},{"location":"predicates/#predicate-no-4f1g","title":"Predicate No. <code>4f1g</code>","text":""},{"location":"predicates/#well-formed-expression_2","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{TranscriptionFactor} \\exists \\mathit{ActivatorProtein} \\forall \\mathit{TargetGene} ((\\mathrm{transcription\\_factor?}(\\mathit{TranscriptionFactor}) \\rightarrow \\mathrm{activates?}(\\mathit{ActivatorProtein}, \\mathit{TargetGene})) \\land \\neg \\mathrm{co\\_located?}(\\mathit{TranscriptionFactor}, \\mathit{TargetGene}))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_2","title":"Interpretation in Natural Language","text":"<p>A protein may activate all genes that aren't co-located with a specific transcription factor.</p>"},{"location":"predicates/#predicate-no-3d4e","title":"Predicate No. <code>3d4e</code>","text":""},{"location":"predicates/#well-formed-expression_3","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{Fluid} \\exists \\mathit{Pipe} (\\neg \\mathrm{incompressible?}(\\mathit{Fluid}) \\land \\mathrm{flows\\_through?}(\\mathit{Fluid}, \\mathit{Pipe}) \\rightarrow \\mathrm{pressure\\_drop?}(\\mathit{Pipe}))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_3","title":"Interpretation in Natural Language","text":"<p>If a compressible fluid flows through a pipe, then the pipe experiences pressure drop.</p>"},{"location":"predicates/#predicate-no-9i0j","title":"Predicate No. <code>9i0j</code>","text":""},{"location":"predicates/#well-formed-expression_4","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{Gene}\\ \\exists \\mathit{Protein}\\ \\forall \\mathit{DNARegion}\\ ((\\mathrm{inhibits\\_proliferation?}(\\mathit{Gene}) \\,\\land\\, \\mathrm{binds\\_to?}(\\mathit{Protein}, \\mathit{DNARegion})) \\rightarrow \\neg \\mathrm{regulates\\_binding?}(\\mathit{Gene}, \\mathit{Protein}, \\mathit{DNARegion}))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_4","title":"Interpretation in Natural Language","text":"<p>A gene that stops cell growth and a protein exist where the gene never regulates the protein binding to DNA.</p>"},{"location":"predicates/#predicate-no-6q7r","title":"Predicate No. <code>6q7r</code>","text":""},{"location":"predicates/#well-formed-expression_5","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{System} \\forall \\mathit{VibrationDamper} \\forall \\mathit{StandardComponent} (\\mathrm{vibration\\_sensitive?}(\\mathit{System}) \\rightarrow (\\mathrm{vibration\\_damper?}(\\mathit{VibrationDamper}) \\land \\neg \\mathrm{replaced\\_by?}(\\mathit{StandardComponent}, \\mathit{VibrationDamper})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_5","title":"Interpretation in Natural Language","text":"<p>In a vibration-sensitive system, a vibration damper is used without replacing any standard component.</p>"},{"location":"predicates/#predicate-no-9j0k","title":"Predicate No. <code>9j0k</code>","text":""},{"location":"predicates/#well-formed-expression_6","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{Reactor} \\forall \\mathit{Condition} (\\neg \\mathrm{cstr?}(\\mathit{Reactor}) \\lor (\\mathrm{high\\_pressure?}(\\mathit{Condition}) \\rightarrow \\mathrm{suitable\\_for?}(\\mathit{Reactor}, \\mathit{Condition})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_6","title":"Interpretation in Natural Language","text":"<p>A reactor exists that either isn't a continuous stirred-tank reactor (CSTR), or is suitable for all high pressure conditions.</p>"},{"location":"predicates/#predicate-no-2e3f","title":"Predicate No. <code>2e3f</code>","text":""},{"location":"predicates/#well-formed-expression_7","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Protein} (\\neg \\mathrm{membrane\\_bound\\_receptor?}(\\mathit{Protein}) \\lor \\exists \\mathit{Molecule} (\\mathrm{interacts\\_with?}(\\mathit{Protein}, \\mathit{Molecule}) \\land \\neg \\mathrm{signaling\\_molecule?}(\\mathit{Molecule})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_7","title":"Interpretation in Natural Language","text":"<p>Every protein is either not a membrane-bound receptor, or it interacts with a molecule that is not a signaling molecule.</p>"},{"location":"predicates/#predicate-no-f3a9","title":"Predicate No. <code>f3a9</code>","text":""},{"location":"predicates/#well-formed-expression_8","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Course} \\exists \\mathit{Module} (\\neg \\mathrm{mandatory?}(\\mathit{Course}) \\lor (\\mathrm{advanced?}(\\mathit{Module}) \\rightarrow \\neg \\mathrm{requires?}(\\mathit{Course}, \\mathit{Module})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_8","title":"Interpretation in Natural Language","text":"<p>Either a course is optional, or it doesn't require advanced modules.</p>"},{"location":"predicates/#predicate-no-6d7e","title":"Predicate No. <code>6d7e</code>","text":""},{"location":"predicates/#well-formed-expression_9","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{GrowthProtein} \\forall \\mathit{Receptor} (\\mathrm{receptor?}(\\mathit{Receptor}) \\rightarrow (\\mathrm{growth\\_protein?}(\\mathit{GrowthProtein}) \\lor \\neg \\mathrm{inhibits?}(\\mathit{GrowthProtein}, \\mathit{Receptor})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_9","title":"Interpretation in Natural Language","text":"<p>A protein involved in cell growth exists that does not inhibit any receptor protein.</p>"},{"location":"predicates/#predicate-no-1b2c","title":"Predicate No. <code>1b2c</code>","text":""},{"location":"predicates/#well-formed-expression_10","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{Process}\\ \\forall \\mathit{Material}\\ (\\neg \\mathrm{corrosive?}(\\mathit{Material}) \\land (\\mathrm{high\\_temperature?}(\\mathit{Process}) \\rightarrow \\mathrm{compatible?}(\\mathit{Process}, \\mathit{Material})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_10","title":"Interpretation in Natural Language","text":"<p>A high-temperature process exists that is compatible with all non-corrosive materials.</p>"},{"location":"predicates/#predicate-no-2b91","title":"Predicate No. <code>2b91</code>","text":""},{"location":"predicates/#well-formed-expression_11","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{PortCity} \\exists \\mathit{ExportCountry} \\forall \\mathit{DestinationCity} (\\mathrm{major\\_port?}(\\mathit{PortCity}) \\land \\mathrm{located\\_in?}(\\mathit{PortCity},\\mathit{ExportCountry}) \\land \\mathrm{exports\\_to?}(\\mathit{ExportCountry}, \\mathit{DestinationCity}) \\land \\neg \\mathrm{located\\_in?}(\\mathit{DestinationCity},\\mathit{ExportCountry}))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_11","title":"Interpretation in Natural Language","text":"<p>A country with a major port may ship to any city outside its borders.</p>"},{"location":"predicates/#predicate-no-1l2m","title":"Predicate No. <code>1l2m</code>","text":""},{"location":"predicates/#well-formed-expression_12","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{Reaction} \\forall \\mathit{Product} \\forall \\mathit{Impurity} (\\mathrm{high\\_yield?}(\\mathit{Reaction}) \\rightarrow (\\neg \\mathrm{contains\\_impurity?}(\\mathit{Product}, \\mathit{Impurity}) \\land \\mathrm{produces?}(\\mathit{Reaction}, \\mathit{Product})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_12","title":"Interpretation in Natural Language","text":"<p>Some high-yield reaction produces products that contain no impurities.</p>"},{"location":"predicates/#predicate-no-8h9i","title":"Predicate No. <code>8h9i</code>","text":""},{"location":"predicates/#well-formed-expression_13","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Protein} \\exists \\mathit{NLS} (\\neg \\mathrm{nuclear\\_protein?}(\\mathit{Protein}) \\lor (\\mathrm{nls?}(\\mathit{NLS}) \\rightarrow \\neg \\mathrm{contains\\_nls?}(\\mathit{Protein}, \\mathit{NLS})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_13","title":"Interpretation in Natural Language","text":"<p>All proteins either aren't nuclear, or they don't have a nuclear signal.</p>"},{"location":"predicates/#predicate-no-8a9b","title":"Predicate No. <code>8a9b</code>","text":""},{"location":"predicates/#well-formed-expression_14","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{Drug} \\exists \\mathit{Target} \\forall \\mathit{Inhibitor} (\\mathrm{therapeutic\\_effect?}(\\mathit{Drug}, \\mathit{Target}) \\land (\\mathrm{known\\_inhibitor?}(\\mathit{Inhibitor}) \\rightarrow \\neg \\mathrm{inhibited\\_by?}(\\mathit{Target}, \\mathit{Inhibitor})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_14","title":"Interpretation in Natural Language","text":"<p>A drug works with a target that isn't blocked by any known inhibitor.</p>"},{"location":"predicates/#predicate-no-4e5f","title":"Predicate No. <code>4e5f</code>","text":""},{"location":"predicates/#well-formed-expression_15","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Fluid1} \\forall \\mathit{Fluid2} (\\neg \\mathrm{miscible?}(\\mathit{Fluid1}, \\mathit{Fluid2}) \\rightarrow (\\mathrm{liquid?}(\\mathit{Fluid1}) \\lor \\mathrm{gas?}(\\mathit{Fluid2})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_15","title":"Interpretation in Natural Language","text":"<p>If two fluids don't mix, then one is a liquid, and the other is a gas.</p>"},{"location":"predicates/#predicate-no-7h8i","title":"Predicate No. <code>7h8i</code>","text":""},{"location":"predicates/#well-formed-expression_16","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{Reaction} \\exists \\mathit{Inhibitor} (\\mathrm{first\\_order\\_reaction?}(\\mathit{Reaction}) \\land \\neg \\mathrm{catalyst?}(\\mathit{Inhibitor}) \\rightarrow \\mathrm{inhibited\\_by?}(\\mathit{Reaction}, \\mathit{Inhibitor}))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_16","title":"Interpretation in Natural Language","text":"<p>Some first-order reaction is inhibited by a non-catalyst compound.</p>"},{"location":"predicates/#predicate-no-d93a","title":"Predicate No. <code>d93a</code>","text":""},{"location":"predicates/#well-formed-expression_17","title":"Well Formed Expression","text":"<p>\\(\\forall X \\exists Y (\\neg \\text{man?}(X) \\lor (\\text{word?}(Y) \\rightarrow \\text{do\\_honour?}(X, Y)))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_17","title":"Interpretation in Natural Language","text":"<p>Either you are not a man enough or you\u2019d honour your word.</p>"},{"location":"predicates/#predicate-no-b8d3","title":"Predicate No. <code>b8d3</code>","text":""},{"location":"predicates/#well-formed-expression_18","title":"Well Formed Expression","text":"<p>\\(\\forall X \\forall Y (\\text{likes?}(X, Y) \\rightarrow \\exists Z (\\text{knows?}(X, Z) \\land \\text{vouch?}(Z, Y)))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_18","title":"Interpretation in Natural Language","text":"<p>If someone likes another, they know someone who'd vouch for them.</p>"},{"location":"predicates/#predicate-no-2w3x","title":"Predicate No. <code>2w3x</code>","text":""},{"location":"predicates/#well-formed-expression_19","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Component} \\forall \\mathit{Condition} (\\mathrm{precision\\_machined?}(\\mathit{Component}) \\land \\neg \\mathrm{high\\_temperature?}(\\mathit{Condition}) \\rightarrow \\mathrm{suitable\\_for?}(\\mathit{Component}, \\mathit{Condition}))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_19","title":"Interpretation in Natural Language","text":"<p>All precision-machined components are suitable for non-high-temperature operating conditions.</p>"},{"location":"predicates/#predicate-no-5c8d","title":"Predicate No. <code>5c8d</code>","text":""},{"location":"predicates/#well-formed-expression_20","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Protein} \\exists \\mathit{Ligand} (\\neg \\mathrm{signaling\\_protein?}(\\mathit{Protein}) \\lor (\\mathrm{ligand?}(\\mathit{Ligand}) \\rightarrow \\mathrm{binds\\_to?}(\\mathit{Protein}, \\mathit{Ligand})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_20","title":"Interpretation in Natural Language","text":"<p>Every protein either isn't a signaling protein, or it binds to some ligand.</p>"},{"location":"predicates/#predicate-no-4o5p","title":"Predicate No. <code>4o5p</code>","text":""},{"location":"predicates/#well-formed-expression_21","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Material} \\forall \\mathit{Process} (\\mathrm{ductile?}(\\mathit{Material}) \\land \\neg \\mathrm{casting\\_process?}(\\mathit{Process}) \\rightarrow \\mathrm{suitable\\_for?}(\\mathit{Material}, \\mathit{Process}))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_21","title":"Interpretation in Natural Language","text":"<p>All ductile materials are suitable for any non-casting manufacturing process.</p>"},{"location":"predicates/#predicate-no-7d6f","title":"Predicate No. <code>7d6f</code>","text":""},{"location":"predicates/#well-formed-expression_22","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{City} \\forall \\mathit{Destination} (\\mathrm{tourist\\_destination?}(\\mathit{Destination}) \\rightarrow (\\mathrm{coastal\\_city?}(\\mathit{City}) \\lor \\neg \\mathrm{more\\_popular?}(\\mathit{City}, \\mathit{Destination})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_22","title":"Interpretation in Natural Language","text":"<p>There is a city that is either a coastal city, or it's less popular than all tourist destinations.</p>"},{"location":"predicates/#predicate-no-c4b2","title":"Predicate No. <code>c4b2</code>","text":""},{"location":"predicates/#well-formed-expression_23","title":"Well Formed Expression","text":"<p>\\(\\exists X \\forall Y \\forall Z (\\text{teacher?}(X) \\land \\text{student\\_subject?}(Y, Z) \\rightarrow \\text{teaches?}(X, Y, Z))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_23","title":"Interpretation in Natural Language","text":"<p>There is a teacher who teaches every student every subject.</p>"},{"location":"predicates/#predicate-no-5f6g","title":"Predicate No. <code>5f6g</code>","text":""},{"location":"predicates/#well-formed-expression_24","title":"Well Formed Expression","text":"<p>\\(\\exists \\mathit{Reaction} \\forall \\mathit{Product} \\forall \\mathit{Inhibitor} (\\mathrm{produces?}(\\mathit{Reaction}, \\mathit{Product}) \\land \\neg \\mathrm{inhibited\\_by?}(\\mathit{Product}, \\mathit{Inhibitor}))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_24","title":"Interpretation in Natural Language","text":"<p>A reaction exists where all products it makes are not inhibited by any inhibitor.</p>"},{"location":"predicates/#predicate-no-2m3n","title":"Predicate No. <code>2m3n</code>","text":""},{"location":"predicates/#well-formed-expression_25","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Machine} (\\neg \\mathrm{precision\\_instrument?}(\\mathit{Machine}) \\lor \\exists \\mathit{Component} (\\mathrm{high\\_strength\\_alloy?}(\\mathit{Component}) \\land \\mathrm{uses?}(\\mathit{Machine}, \\mathit{Component})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_25","title":"Interpretation in Natural Language","text":"<p>All machines either aren't precision instruments, or they use a high-strength alloy component.</p>"},{"location":"predicates/#predicate-no-1b4c","title":"Predicate No. <code>1b4c</code>","text":""},{"location":"predicates/#well-formed-expression_26","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Restaurant} (\\mathrm{popular?}(\\mathit{Restaurant}) \\rightarrow \\exists \\mathit{Dish} (\\mathrm{vegetarian?}(\\mathit{Dish}) \\land \\mathrm{serves?}(\\mathit{Restaurant}, \\mathit{Dish})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_26","title":"Interpretation in Natural Language","text":"<p>Every popular restaurant serves at least one vegetarian dish.</p>"},{"location":"predicates/#predicate-no-8s9t","title":"Predicate No. <code>8s9t</code>","text":""},{"location":"predicates/#well-formed-expression_27","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Machine} \\exists \\mathit{Component} (\\neg \\mathrm{requires\\_maintenance?}(\\mathit{Machine}, \\mathit{Component}) \\lor (\\mathrm{high\\_wear?}(\\mathit{Component}) \\rightarrow \\mathrm{high\\_maintenance\\_frequency?}(\\mathit{Machine})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_27","title":"Interpretation in Natural Language","text":"<p>Every machine either has a component it rarely maintains, or it needs frequent maintenance due to some high-wear component.</p>"},{"location":"predicates/#predicate-no-3n4o","title":"Predicate No. <code>3n4o</code>","text":""},{"location":"predicates/#well-formed-expression_28","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{Component} \\exists \\mathit{AssemblyStation} \\exists \\mathit{Tool} (\\mathrm{compatible?}(\\mathit{Tool}, \\mathit{Component}) \\land \\mathrm{uses\\_tool?}(\\mathit{AssemblyStation}, \\mathit{Tool}, \\mathit{Component}))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_28","title":"Interpretation in Natural Language","text":"<p>All components are controlled by an assembly station through a compatible tool.</p>"},{"location":"predicates/#predicate-no-0u1v","title":"Predicate No. <code>0u1v</code>","text":""},{"location":"predicates/#well-formed-expression_29","title":"Well Formed Expression","text":"<p>\\(\\forall \\mathit{QualityControlSystem} \\exists \\mathit{CriticalDefect} \\forall \\mathit{Sensor} (\\mathrm{automated?}(\\mathit{QualityControlSystem}) \\rightarrow (\\mathrm{critical?}(\\mathit{CriticalDefect}) \\land \\neg \\mathrm{detects?}(\\mathit{Sensor}, \\mathit{CriticalDefect})))\\)</p>"},{"location":"predicates/#interpretation-in-natural-language_29","title":"Interpretation in Natural Language","text":"<p>Automated quality control systems have blind spots for critical defects.</p>"}]}